# Global settings.
shutdown_timeout: "3m"
logger:
  level: INFO
  format: json
http:
  enabled: false


input:
  aws_s3:
    bucket: "${RP_CONNECT_BUCKET:}"
    region: "${RP_CONNECT_REGION:us-east-1}"
    prefix: "${RP_CONNECT_PREFIX:*.tar.gz}"
    delete_objects: ${RP_CONNECT_DELETE_OBJECTS:false}
    credentials:
      from_ec2_role: ${RP_CONNECT_USE_EC2_ROLE:false}
      role: "${RP_CONNECT_ROLE:}"
    scanner:
      decompress:
        algorithm: "gzip"
        into:
          tar: {}

pipeline:
  processors:
    # Drop some metadata that comes from tarballs or bogus events from S3.
    - mapping: |
        if !content().has_prefix("{") || metadata("s3_key").or("").length() == 0 { deleted() }
    # Help aid debugging by logging info on the object we're processing.
    - log:
        message: processing ${! metadata("tar_name") } from s3://${! metadata("s3_bucket") }/${! metadata("s3_key") }
    # Split out the newline-delimitted JSON into individual messages.
    - unarchive:
        format: json_documents
    # Pull out relevant content and assign a message key.
    - mapping: |
        root.text = this.abstract
        root.metadata.title = this.name
        root.metadata.url = this.url
        root.metadata.event = this.event
        meta key = this.event.identifier

output:
  label: redpanda
  kafka_franz:
    seed_brokers: [ "${RP_CONNECT_BROKER:127.0.0.1:9092}" ]
    topic: "${RP_CONNECT_TOPIC:wikipedia}"
    key: ${! metadata("key") }
    tls:
      enabled: ${RP_CONNECT_TLS:false}
    sasl:
      - mechanism: "${RP_CONNECT_SASL_MECH:none}"
        username: "${RP_CONNECT_USERNAME:}"
        password: "${RP_CONNECT_PASSWORD:}"
    batching:
      byte_size: 1048576
      period: "1s"

# Test our pipeline logic.
tests:
  - name: Good input test (shows the shape of the Wikipedia data)
    input_batch:
      - content: |
          { "abstract": "txt1", "name": "n1", "url": "x", "event": { "identifier": "1" } }
          { "abstract": "txt2", "name": "n2", "url": "y", "event": { "identifier": "2" } }
          { "abstract": "txt3", "name": "n3", "url": "z", "event": { "identifier": "3" } }
        metadata:
          s3_key: some-key
    output_batches:
      - # Should have 3 messages from the single input_batch.
        - json_equals: |
            { "text": "txt1", "metadata": { "title": "n1", "url": "x", "event": { "identifier": "1" } } }
          metadata_equals:
            s3_key: some-key
        - json_equals: |
            { "text": "txt2", "metadata": { "title": "n2", "url": "y", "event": { "identifier": "2" } } }
          metadata_equals:
            s3_key: some-key
        - json_equals: |
            { "text": "txt3", "metadata": { "title": "n3", "url": "z", "event": { "identifier": "3" } } }
          metadata_equals:
            s3_key: some-key
  - name: Drops bad data
    input_batch:
      - content: |
          { "abstract": "txt1", "name": "n11", "url": "http://localhost", "event": { "identifier": "1" } }
        metadata: {} # missing s3_key
      - content: |
          "this is not json content!"
        metadata:
          s3_key: some-key
    output_batches: [] # should be empty!
