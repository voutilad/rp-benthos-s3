shutdown_timeout: "5m"
logger:
  level: INFO
  format: json

input:
  aws_s3:
    bucket: "${RP_CONNECT_BUCKET}"
    region: "${RP_CONNECT_REGION}"
    prefix: "${RP_CONNECT_PREFIX:}"
    delete_objects: ${RP_CONNECT_DELETE_OBJECTS:false}
    credentials:
      from_ec2_role: ${RP_CONNECT_USE_EC2_ROLE:false}
      role: "${RP_CONNECT_ROLE:}"
    scanner:
      decompress:
        algorithm: "gzip"
        into:
          tar: {}

pipeline:
  processors:
    # Drop some metadata that comes from tarballs or bogus events from S3.
    - mapping: |
        if !content().has_prefix("{") || metadata("s3_key").or("").length() == 0 { deleted() }
    # Help aid debugging by logging info on the object we're processing.
    - log:
        level: INFO
        message: |
          processing ${! metadata("tar_name") } from s3://${! metadata("s3_bucket") }/${! metadata("s3_key") }
    # Split out the newline-delimitted JSON into individual messages.
    - unarchive:
        format: "json_documents"
    # Pull out relevant content and assign a message key.
    - mapping: |
        root.text = this.abstract
        root.metadata.title = this.name
        root.metadata.url = this.url
        root.metadata.event = this.event
        meta key = this.event.identifier

output:
  kafka_franz:
    seed_brokers: [ "${RP_CONNECT_BROKER:127.0.0.1:9092}" ]
    topic: "${RP_CONNECT_TOPIC:wikipedia}"
    key: ${! metadata("key") }
    tls:
      enabled: ${RP_CONNECT_TLS:false}
    sasl:
      - mechanism: "${RP_CONNECT_SASL_MECH:none}"
        username: "${RP_CONNECT_USERNAME:}"
        password: "${RP_CONNECT_PASSWORD:}"
    batching:
      byte_size: 1048576
      period: "1s"
